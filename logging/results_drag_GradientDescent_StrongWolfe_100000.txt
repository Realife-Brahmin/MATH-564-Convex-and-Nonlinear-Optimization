****************************
Solver run has concluded.
Function solved for: drag()
This was a function minimization problem.
Cause(s) for stopping:
Barely changing fval
Convergence achieved in 64010 iterations ðŸ˜„
****************************
Method Used: GradientDescent
Linesearch method used: StrongWolfe
****************************
Optimal function value = 0.18857528457345096
***************************
Optimal Nonzero Variables:
xâ˜†[1] = 0.3475598882283785
xâ˜†[2] = 0.3541253320702683
xâ˜†[3] = 0.36057951242678365
xâ˜†[4] = 0.36693131817143043
xâ˜†[5] = 0.37318840248712276
xâ˜†[6] = 0.37935742221996793
xâ˜†[7] = 0.38544421860021255
xâ˜†[8] = 0.3914539566420702
xâ˜†[9] = 0.3973912346697395
xâ˜†[10] = 0.4032601721904796
xâ˜†[11] = 0.4090644784554113
xâ˜†[12] = 0.4148075110782831
xâ˜†[13] = 0.4204923220903982
xâ˜†[14] = 0.42612169719620696
xâ˜†[15] = 0.43169818871404747
xâ˜†[16] = 0.43722414277134813
xâ˜†[17] = 0.44270172309728345
xâ˜†[18] = 0.4481329307914452
xâ˜†[19] = 0.4535196215716516
xâ˜†[20] = 0.45886352053776025
xâ˜†[21] = 0.46416623577755206
xâ˜†[22] = 0.4694292685305124
xâ˜†[23] = 0.47465402402057877
xâ˜†[24] = 0.47984181999317116
xâ˜†[25] = 0.4849938940219431
xâ˜†[26] = 0.4901114106539955
xâ˜†[27] = 0.4951954675383638
xâ˜†[28] = 0.5002471011358898
xâ˜†[29] = 0.5052672906607555
xâ˜†[30] = 0.5102569635439335
xâ˜†[31] = 0.5152169986885191
xâ˜†[32] = 0.5201482301755523
xâ˜†[33] = 0.5250514505567893
xâ˜†[34] = 0.5299274136737014
xâ˜†[35] = 0.5347768376295582
xâ˜†[36] = 0.5396004070663994
xâ˜†[37] = 0.5443987748011638
xâ˜†[38] = 0.5491725648009886
xâ˜†[39] = 0.5539223734973884
xâ˜†[40] = 0.5586487713431234
xâ˜†[41] = 0.5633523050640109
xâ˜†[42] = 0.5680334984041889
xâ˜†[43] = 0.5726928535742097
xâ˜†[44] = 0.5773308532095502
xâ˜†[45] = 0.581947960189236
xâ˜†[46] = 0.586544620114208
xâ˜†[47] = 0.5911212611065885
xâ˜†[48] = 0.5956782956659594
xâ˜†[49] = 0.6002161206385925
xâ˜†[50] = 0.60473511881197
xâ˜†[51] = 0.6092356593614509
xâ˜†[52] = 0.6137180982658293
xâ˜†[53] = 0.6181827791848497
xâ˜†[54] = 0.6226300341553315
xâ˜†[55] = 0.6270601838879447
xâ˜†[56] = 0.6314735387679066
xâ˜†[57] = 0.6358703989570867
xâ˜†[58] = 0.6402510548148431
xâ˜†[59] = 0.6446157871906019
xâ˜†[60] = 0.6489648687299212
xâ˜†[61] = 0.653298563410012
xâ˜†[62] = 0.6576171271844541
xâ˜†[63] = 0.6619208080849873
xâ˜†[64] = 0.6662098470514494
xâ˜†[65] = 0.6704844778509831
xâ˜†[66] = 0.6747449275516303
xâ˜†[67] = 0.6789914165326113
xâ˜†[68] = 0.6832241592207661
xâ˜†[69] = 0.6874433638609387
xâ˜†[70] = 0.6916492330365022
xâ˜†[71] = 0.6958419637566868
xâ˜†[72] = 0.7000217476544637
xâ˜†[73] = 0.7041887715464367
xâ˜†[74] = 0.7083432171848459
xâ˜†[75] = 0.7124852612271217
xâ˜†[76] = 0.716615076278626
xâ˜†[77] = 0.7207328305312163
xâ˜†[78] = 0.7248386875023002
xâ˜†[79] = 0.7289328072401446
xâ˜†[80] = 0.7330153454701813
xâ˜†[81] = 0.7370864540541687
xâ˜†[82] = 0.7411462813477824
xâ˜†[83] = 0.7451949722752744
xâ˜†[84] = 0.7492326682739097
xâ˜†[85] = 0.7532595072223632
xâ˜†[86] = 0.7572756241005812
xâ˜†[87] = 0.7612811508372666
xâ˜†[88] = 0.7652762158890697
xâ˜†[89] = 0.7692609454828602
xâ˜†[90] = 0.7732354625024261
xâ˜†[91] = 0.7771998875465723
xâ˜†[92] = 0.7811543382762806
xâ˜†[93] = 0.7850989298292746
xâ˜†[94] = 0.7890337749891285
xâ˜†[95] = 0.7929589842969086
xâ˜†[96] = 0.7968746656016678
xâ˜†[97] = 0.8007809247776925
xâ˜†[98] = 0.8046778653198252
xâ˜†[99] = 0.8085655887283151
xâ˜†[100] = 0.812444194664191
xâ˜†[101] = 0.81631378016776
xâ˜†[102] = 0.8201744411118883
xâ˜†[103] = 0.8240262708690764
xâ˜†[104] = 0.8278693614012047
xâ˜†[105] = 0.8317038024940624
xâ˜†[106] = 0.8355296824325462
xâ˜†[107] = 0.8393470878782602
xâ˜†[108] = 0.8431561036675159
xâ˜†[109] = 0.8469568133384078
xâ˜†[110] = 0.8507492985554733
xâ˜†[111] = 0.8545336395555467
xâ˜†[112] = 0.858309915192344
xâ˜†[113] = 0.8620782030166935
xâ˜†[114] = 0.8658385788313632
xâ˜†[115] = 0.8695911172946039
xâ˜†[116] = 0.8733358915950049
xâ˜†[117] = 0.8770729739959535
xâ˜†[118] = 0.8808024350615479
xâ˜†[119] = 0.8845243444002375
xâ˜†[120] = 0.8882387704756456
xâ˜†[121] = 0.891945780070443
xâ˜†[122] = 0.8956454400632499
xâ˜†[123] = 0.8993378142373555
xâ˜†[124] = 0.903022968074337
xâ˜†[125] = 0.9067009615358008
xâ˜†[126] = 0.9103718611396163
xâ˜†[127] = 0.9140357195705496
xâ˜†[128] = 0.917692610906607
xâ˜†[129] = 0.9213425688546633
xâ˜†[130] = 0.9249856944926342
xâ˜†[131] = 0.9286219718524731
xâ˜†[132] = 0.9322515772126678
xâ˜†[133] = 0.9358743722379731
xâ˜†[134] = 0.9394907168830057
xâ˜†[135] = 0.9431001912392004
xâ˜†[136] = 0.946703565346903
xâ˜†[137] = 0.9502998298494623
xâ˜†[138] = 0.953890568307457
xâ˜†[139] = 0.9574736727238068
xâ˜†[140] = 0.9610521533823297
xâ˜†[141] = 0.9646221103590812
xâ˜†[142] = 0.968188702970648
xâ˜†[143] = 0.9717455741669324
xâ˜†[144] = 0.9753005160499872
xâ˜†[145] = 0.9788445690380589
xâ˜†[146] = 0.9823877927826599
xâ˜†[147] = 0.9859196629797095
xâ˜†[148] = 0.9894506899909109
xâ˜†[149] = 0.9929713774667386
xâ˜†[150] = 0.9964894801239264
***************************
Number of fevals = 950558
Number of gevals = 253114
Number of evals = 38917658
**************************************************
Tolerance critera used:
dftol = 1.0e-12
