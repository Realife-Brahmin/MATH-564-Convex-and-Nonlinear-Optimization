# module objective

using DataFrames
using Symbolics

include("helperFunctions.jl");

# export objFun
"""
    objFun(df::DataFrame; getGradientToo=true, getFunctionsToo=true,
        getVariables=true, verbose=false)

Compute the objective function for a decaying sinusoidal model.

# Arguments

- `df::DataFrame`: A DataFrame with columns `t` and `V` where `t` represents time in seconds and `V` represents voltage in millivolts.

# Keyword Arguments

- `getGradientToo::Bool=true`: If true, the function will also compute and return the gradient of the objective function.

- `getFunctionsToo::Bool=true`: If true, the function will also return the numerical (compiled) versions of the symbolic objective function (and its gradient if `getGradientToo` is true).

- `getVariables::Bool=true`: Not used in the provided function code.

- `verbose::Bool=false`: If true, will print out detailed information during the computation. Not implemented in the provided function code.

# Returns

- `fobj`: The symbolic representation of the objective function.

- `∇fobj` (optional): The symbolic gradient of the objective function. Only returned if `getGradientToo=true`.

- `fnum` (optional): The compiled numerical function of the objective function. Only returned if `getFunctionsToo=true`.

- `∇fnum` (optional): The compiled numerical function of the gradient. Only returned if `getGradientToo=true` and `getFunctionsToo=true`.

- `x`: The symbolic vector of parameters used in the model.

# Example

```julia
df = DataFrame(t=[0.1, 0.2, 0.3], V=[0.5, 0.4, 0.3])
fobj, ∇fobj, x = objFun(df)
"""
function objFun(df::DataFrame;
    getGradientToo::Bool=true, 
    getFunctionsToo::Bool=true,
    getVariables::Bool=true, 
    verbose::Bool=false)

    N = size(df, 1)
    @variables t, A₀, A, τ, ω, α, ϕ
    x = [A₀, A, τ, ω, α, ϕ] 

    fsig = A₀ + A*exp(-t/τ)sin((ω+α*t)t + ϕ)
    fobj = mean([(df.V[i] - substitute(fsig, t => df.t[i]))^2 for i in 1:N])

    if getGradientToo
        # Calculate the gradient for each squared difference
        gradients = [Symbolics.gradient(fobj[i], x) 
        for i ∈ eachindex(fobj)]   
        # Sum up the gradients to get the overall gradient
        ∇fobj = sum(gradients)
        if getFunctionsToo
            fnum = build_function(fobj, x, expression=Val{false})
            ∇fnum = build_function(∇fobj, x, expression=Val{false})
            return fobj, ∇fobj, fnum, ∇fnum, x
        else
            return fobj, ∇fobj, x
        end
    else
        if getFunctionsToo
            fnum = build_function(fobj, x, expression=Val{false})
            return fobj, fnum, x
        else
            return fobj, x
        end
    end
end

"""
    computeCost(pr::NamedTuple, xnow::Vector{Float64}, t::Float64; getGradientToo::Bool=true, verbose::Bool=false)

Compute the sum of squared errors (SSE) between the observed dataset `df` (from `pr.df`) and the values generated by the objective function `pr.objective` for the provided parameters `xnow` and time `t`.

# Arguments
- `pr::NamedTuple`: A named tuple containing necessary information, including:
    - `objective`: Name of the objective function.
    - `df`: Dataset with observed values (`V`) and times (`t`).
- `xnow::Vector{Float64}`: Current set of parameter values.
- `t::Float64`: Current time value.

# Keyword Arguments
- `getGradientToo::Bool=true`: Whether to compute the gradient of the SSE with respect to `xnow`.
- `verbose::Bool=false`: Whether to display additional information during computation.

# Returns
- `Fval`: The computed SSE.
- If `getGradientToo` is set to true, the function also returns `Gval`, which represents the gradient of the SSE.

# Example
```julia
pr = (objective="someFunction", df=(t=[1,2,3], V=[4,5,6]))
xnow = [0.5, 0.5]
t = 1.0
Fval, Gval = computeCost(pr, xnow, t, getGradientToo=true)
"""
function computeCost(pr::NamedTuple, xnow::Vector{Float64}; getGradientToo::Bool=true, verbose::Bool=false)
    
    df = pr.df
    M = length(df.V)
    
    # Calculate SSE
    residuals = [df.V[i] - evaluateFunction(pr, xnow, df.t[i], getGradientToo=false) for i in 1:M]
    Fval = mean(residuals.^2)

    if getGradientToo
        gradient_contributions = [ 
            begin
                fval, gval = evaluateFunction(pr, xnow, df.t[i])
                -2/M * (df.V[i] - fval) * gval
            end
            for i in 1:M
        ]
        Gval = sum(gradient_contributions)
        return Fval, Gval
    else
        return Fval
    end
end

"""
    buildFunctions_DampedSHM(; getGradientToo::Bool=true, printSymbolicEquations::Bool=false, verbose::Bool=false)

Build numerical functions for the damped simple harmonic motion (SHM) system based on symbolic representations using the `Symbolics.jl` package.

# Keyword Arguments
- `getGradientToo::Bool=true`: Whether to compute the gradient of the function along with its symbolic representation.
- `printSymbolicEquations::Bool=false`: If set to true, the symbolic representation of the function (and its gradient, if computed) will be printed to the console.
- `verbose::Bool=false`: Enables additional print statements for debugging and information purposes.

# Returns
- A named tuple containing:
    - `fnum`: A numerical function representation of the damped SHM system.
    - `∇fnum`: If `getGradientToo` is true, a numerical function representation of the gradient of the damped SHM system. Otherwise, it returns `nothing`.

# Example
```julia
functions = buildFunctions_DampedSHM()
val = functions.fnum([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], 0.5)
"""
function buildFunctions_DampedSHM(;
    getGradientToo::Bool=true,
    printSymbolicEquations::Bool=false,
    verbose::Bool=false)

    @variables t, A₀, A, τ, ω, α, ϕ
    x = [A₀, A, τ, ω, α, ϕ] 
    f = A₀ + A*exp(-t/τ)sin((ω+α*t)t + ϕ)
    fnum = build_function(f, x, t, expression=Val{false})
    
    # ∇fnum = nothing
    if getGradientToo
        myprintln(verbose, "I'm getting the gradient as asked.")
        ∇f = Symbolics.gradient(f, x)
        ∇fnum = build_function(∇f, x, t, expression=Val{false})
        ∇fnum = ∇fnum[1] # Only taking the first function from the tuple, no idea why there's a tuple being generated
    else
        ∇fnum = nothing
    end

    if printSymbolicEquations
        println("Function: ", f)
        if getGradientToo
            println("Gradient: ", ∇f)
        end
    end

    return (fnum=fnum, ∇fnum=∇fnum)
end

"""
    dampedSHM(x::Vector{Float64}, t::Float64;
        getGradientToo::Bool=true,
        printSymbolicEquations::Bool=false,
        verbose::Bool=false)

Computes the value of a damped simple harmonic motion (SHM) at a specific vector of parameters `x` and time `t`.

# Arguments
- `x::Vector{Float64}`: A vector of parameters for the damped SHM function.
- `t::Float64`: The time at which the SHM function is evaluated.
- `getGradientToo::Bool=true`: Determines whether the gradient of the function should be computed and returned. Defaults to `true`.
- `printSymbolicEquations::Bool=false`: If `true`, the symbolic expressions for the function and its gradient will be printed to the console. Defaults to `false`.
- `verbose::Bool=false`: If `true`, certain diagnostic messages during the function's execution will be printed. Defaults to `false`.

# Returns
- `fval`: The value of the damped SHM at `x` and `t`.
- `∇fval`: The gradient of the function at `x` and `t`. Only returned if `getGradientToo=true`.

# Example
```julia
value, gradient = dampedSHM([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], 2.5)
"""
function dampedSHM(x::Vector{Float64}, t::Float64;
    getGradientToo::Bool=true,
    printSymbolicEquations::Bool=false,
    verbose::Bool=false)

    fnum, ∇fnum = buildFunctions_DampedSHM(printSymbolicEquations=printSymbolicEquations, verbose=verbose)

    fval = fnum(x, t)
    if getGradientToo 
        ∇fval = ∇fnum(x, t)
        return fval, ∇fval
    else
        return fval
    end
end

"""
    findDirection(pr::NamedTuple, ∇fnow::Vector{Float64}; verbose::Bool=false) -> Vector{Float64}

Compute the search direction for optimization methods based on the provided gradient `∇fnow` and the method specified in `pr.alg.method`.

# Arguments
- `pr::NamedTuple`: A named tuple containing problem configurations. Specifically, it must have `pr.alg.method` which defines the optimization method to be used.
- `∇fnow::Vector{Float64}`: The current gradient of the function to be optimized.

# Keyword Arguments
- `verbose::Bool=false`: Enables additional print statements for debugging and information purposes.

# Returns
- `Vector{Float64}`: The computed search direction.

# Example
```julia
pr = (alg=(method="GradientDescent", ...), ...)
gradient = [1.0, 2.0, 3.0]
direction = findDirection(pr, gradient)
"""
function findDirection(pr::NamedTuple, ∇fnow::Vector{Float64};
    verbose::Bool=false)::Vector{Float64}
    method = pr.alg.method
    n = length(∇fnow)
    if method == "GradientDescent"
        Bₖ = I(n)
        pₖ = -Bₖ*∇fnow
    else 
        @error "Currently not formulated for this method"
    end

    return pₖ
end

"""
    linesearch(pr::NamedTuple, xnow::Vector{Float64}, pₖ::Vector{Float64}; verbose::Bool=false)::Tuple{Float64, Vector{Float64}, Float64}

Performs line search to find an appropriate step size (`α`) that ensures the next parameter value `xnext` satisfies the specified conditions, and returns the objective function value `F` at that point.

# Arguments
- `pr::NamedTuple`: An object containing configurations, data, and algorithm settings.
- `xnow::Vector{Float64}`: The current values of the model parameters.
- `pₖ::Vector{Float64}`: The direction vector for the search.

# Keyword Arguments
- `verbose::Bool`: A flag for printing additional information during execution. Default is `false`.

# Returns
- A tuple containing:
    - `α`: The calculated step size.
    - `x`: The next parameter value `xnow + α*pₖ`.
    - `F`: The objective function value at `x`.

### Notes:
- The specific line search condition to use (e.g., "Armijo" or "StrongWolfe") is specified within the `pr` named tuple.
- This function primarily uses the Armijo condition to determine the step size. 
- It makes use of the `evaluateFunction` and `computeCost` functions.

# Example
```julia
pr = (alg=(linesearch="Armijo", c1=0.1, c2=0.9, ...), ...)
x_values = [1.0, 2.0, 3.0]
direction = [-0.5, -0.5, -0.5]
result = linesearch(pr, x_values, direction, verbose=true)
"""
function linesearch(pr::NamedTuple, xnow::Vector{Float64}, 
    pₖ::Vector{Float64};
    verbose::Bool=false)::Float64
    f = Symbol(pr.objective)
    
    linesearch = pr.alg.linesearch
    c₁ = pr.alg.c1
    c₂ = pr.alg.c2
    α = 1e-8
    β = 1
    diff = β*pₖ
    xnext = xnow+diff
    Fnow, Gnow = computeCost(pr, xnow)
    println("Current value of F, Fnow = $(Fnow)")
    armijoSatisfied = false
    if linesearch == "StrongWolfe"
        # sufficient decrease condition
    elseif linesearch == "Armijo"
        # fnow, ∇fₖ = evaluateFunction(pr, xnow, t)
        while !armijoSatisfied
            diff = β*pₖ
            println("Let's shift x by $(diff)")
            xnext = xnow+diff
            @show Fnext = computeCost(pr, xnext, getGradientToo=false)
            # println(c₁*β*∇fₖ'*pₖ)
            println("To be compared against: $(Fnow + c₁*β*Gnow'*pₖ)")
            if Fnext ≤ Fnow + c₁*β*Gnow'*pₖ
                myprintln(verbose, "Armijo condition satisfied for β = $(β)")
                armijoSatisfied = true
            else
                β /= 2
                myprintln(verbose, "Armijo condition NOT satisfied for β = $(β)")
            end 
        end
    else 
        @error "Unknown linesearch condition"
    end
    
    α = β
    return (α=α, x=xnext, F=Fnext) 
end

"""
    evaluateFunction(pr, x::Vector{Float64}, t::Float64; kwargs...)

Dynamically evaluates a function specified by the `pr.objective` string, given the vector `x`, parameter `t`, and any additional keyword arguments.

# Arguments
- `pr`: An object (typically a NamedTuple) containing configurations. Specifically:
    * `pr.objective`: A string that specifies the name of the function to be evaluated.
- `x::Vector{Float64}`: The input vector for the function.
- `t::Float64`: A parameter required by the function.

# Keyword Arguments
- `kwargs...`: Additional keyword arguments that might be required by the function being evaluated.

# Returns
- The result of the function evaluation.

# Example
```julia
pr = (objective="myFunctionName", ...)
x_values = [1.0, 2.0, 3.0]
t_value = 0.5
result = evaluateFunction(pr, x_values, t_value, arg1=value1, arg2=value2)
"""
function evaluateFunction(pr, x::Vector{Float64}, t::Float64; kwargs...)
    # Convert the string name to a function symbol
    func_sym = Symbol(pr.objective)
    
    # Dynamically call the function with the provided arguments and keyword arguments
    value = eval(:($func_sym($x, $t; $kwargs...)))
    
    return value
end

# end