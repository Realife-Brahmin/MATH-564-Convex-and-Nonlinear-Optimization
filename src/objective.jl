# module objective

using Base.Threads
using DataFrames
using Symbolics

include("helperFunctions.jl");

function buildFunctions_DampedSHM()

    @variables t, A₀, A, τ, ω, α, ϕ
    x = [A₀, A, τ, ω, α, ϕ] 
    f = A₀ + A*exp(-t/τ)sin((ω+α*t)t + ϕ)
    fnum = build_function(f, x, t, expression=Val{false})
    ∇f = Symbolics.gradient(f, x)
    ∇fnum = build_function(∇f, x, t, expression=Val{false})
    ∇fnum = ∇fnum[1] # Only taking the first function from the tuple

    function dampedSHM(x::Vector{Float64},
            t::Float64;
            getGradientToo::Bool=true,printSymbolicEquations::Bool=false,
            verbose::Bool=false)

        if printSymbolicEquations
            println("Function: ", f)
            if getGradientToo
                println("Gradient: ", ∇f)
            end
        end
        
        value = fnum(x, t)
        if getGradientToo
            gradient = ∇fnum(x, t)
            return value, gradient
        else
            return value
        end
    end
    
    return dampedSHM
end

"""
    computeCost(pr::NamedTuple, xnow::Vector{Float64}; getGradientToo::Bool=true, verbose::Bool=false)

Compute the sum of squared errors (SSE) between the observed dataset `df` (from `pr.df`) and the values generated by the objective function `pr.objective` for the provided parameters `xnow`.

# Arguments
- `pr::NamedTuple`: A named tuple containing necessary information, including:
    - `objective`: Name of the objective function.
    - `df`: Dataset with observed values (`V`) and times (`t`).
- `xnow::Vector{Float64}`: Current set of parameter values.

# Keyword Arguments
- `getGradientToo::Bool=true`: Whether to compute the gradient of the SSE with respect to `xnow`.
- `verbose::Bool=false`: Whether to display additional information during computation.

# Returns
- `Fval`: The computed SSE.
- If `getGradientToo` is set to true, the function also returns `Gval`, which represents the gradient of the SSE.

# Example
```julia
pr = (objective="someFunction", df=(t=[1,2,3], V=[4,5,6]))
xnow = [0.5, 0.5]
Fval, Gval = computeCost(pr, xnow, getGradientToo=true)
"""
function computeCost(pr::NamedTuple, xnow::Vector{Float64}; getGradientToo::Bool=true, verbose::Bool=false, log=true)
    
    df = pr.df
    M = length(df.V)
    
    # Calculate SSE
    residuals = [df.V[i] - pr.objective(xnow, df.t[i], getGradientToo=false) for i in 1:M]
    Fval = mean(residuals.^2)

    if getGradientToo
        gradient_contributions = [ 
            begin
                fval, gval = pr.objective( xnow, df.t[i])
                -2/M * (df.V[i] - fval) * gval
            end
            for i in 1:M
        ]
        Gval = sum(gradient_contributions)
        return Fval, Gval
    else
        return Fval
    end
end




# Create your dampedSHM function with your desired settings
dampedSHM = buildFunctions_DampedSHM()

# end